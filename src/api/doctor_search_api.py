"""
API Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¯Ú©ØªØ± Ø¯Ø± Ù¾Ø°ÛŒØ±Ø´Û²Û´
"""
import httpx
import re
from typing import List, Optional, Dict
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

from .models import Doctor


class DoctorSearchAPI:
    """API Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ú©ØªØ±"""
    
    def __init__(self, timeout: int = 15):
        self.timeout = timeout
        self.base_url = "https://www.paziresh24.com"
        self.api_base = "https://apigw.paziresh24.com"
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fa,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    async def search_doctors(self, query: str, city: str = "", specialty: str = "") -> List[Dict]:
        """
        Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¯Ú©ØªØ± Ø¯Ø± Ù¾Ø°ÛŒØ±Ø´Û²Û´
        
        Args:
            query: Ù†Ø§Ù… Ø¯Ú©ØªØ± ÛŒØ§ Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ
            city: Ø´Ù‡Ø± (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            specialty: ØªØ®ØµØµ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            
        Returns:
            Ù„ÛŒØ³Øª Ø¯Ú©ØªØ±Ù‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡
        """
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                # Ø³Ø§Ø®Øª URL Ø¬Ø³ØªØ¬Ùˆ
                search_url = f"{self.base_url}/search"
                params = {
                    'q': query,
                    'city': city,
                    'specialty': specialty
                }
                
                response = await client.get(search_url, params=params, headers=self.headers)
                response.raise_for_status()
                
                # Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† Ù†ØªØ§ÛŒØ¬
                soup = BeautifulSoup(response.text, 'html.parser')
                doctors = self._parse_search_results(soup)
                
                return doctors
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¯Ú©ØªØ±: {e}")
            return []
    
    async def get_doctor_from_url(self, doctor_url: str) -> Optional[Doctor]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ Ø¯Ú©ØªØ± Ø§Ø² URL ØµÙØ­Ù‡
        
        Args:
            doctor_url: Ù„ÛŒÙ†Ú© ØµÙØ­Ù‡ Ø¯Ú©ØªØ±
            
        Returns:
            Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ Ø¯Ú©ØªØ± ÛŒØ§ None
        """
        try:
            # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ URL
            if not self._validate_doctor_url(doctor_url):
                return None
            
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                # Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡ Ø¯Ú©ØªØ±
                response = await client.get(doctor_url, headers=self.headers)
                response.raise_for_status()
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡
                basic_info = await self._extract_basic_info(response.text, doctor_url)
                if not basic_info:
                    return None
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª API
                api_info = await self._extract_api_info(client, doctor_url, basic_info['slug'])
                if not api_info:
                    return None
                
                # ØªØ±Ú©ÛŒØ¨ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
                doctor_data = {**basic_info, **api_info}
                
                return Doctor(
                    name=doctor_data['name'],
                    slug=doctor_data['slug'],
                    center_id=str(doctor_data['center_id']),
                    service_id=str(doctor_data['service_id']),
                    user_center_id=str(doctor_data['user_center_id']),
                    terminal_id=str(doctor_data['terminal_id']),
                    specialty=doctor_data.get('specialty', ''),
                    center_name=doctor_data.get('center_name', ''),
                    center_address=doctor_data.get('center_address', ''),
                    center_phone=doctor_data.get('center_phone', ''),
                    is_active=True
                )
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ú©ØªØ±: {e}")
            return None
    
    def _validate_doctor_url(self, url: str) -> bool:
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ URL Ø¯Ú©ØªØ±"""
        pattern = r'https?://(?:www\.)?paziresh24\.com/dr/[^/]+/?'
        return re.match(pattern, url) is not None
    
    async def _extract_basic_info(self, html: str, url: str) -> Optional[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡ Ø§Ø² HTML"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ slug Ø§Ø² URL
            slug_match = re.search(r'/dr/([^/]+)/?', url)
            if not slug_match:
                return None
            slug = slug_match.group(1)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ø¯Ú©ØªØ±
            name_element = (
                soup.find('h1') or 
                soup.find('title') or
                soup.find('meta', {'property': 'og:title'})
            )
            
            if name_element:
                if name_element.name == 'meta':
                    name = name_element.get('content', '')
                else:
                    name = name_element.get_text().strip()
            else:
                name = f"Ø¯Ú©ØªØ± {slug}"
            
            # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ø§Ø¶Ø§ÙÛŒ
            name = re.sub(r'^(Ø¯Ú©ØªØ±|Dr\.?)\s*', '', name)
            name = re.sub(r'\s*-.*$', '', name)
            name = name.strip()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ®ØµØµ
            specialty = self._extract_specialty(soup)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±Ú©Ø²
            center_info = self._extract_center_info(soup)
            
            return {
                'name': name,
                'slug': slug,
                'specialty': specialty,
                **center_info
            }
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡: {e}")
            return None
    
    def _extract_specialty(self, soup: BeautifulSoup) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ®ØµØµ Ø¯Ú©ØªØ±"""
        try:
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ ØªØ®ØµØµ
            specialty_keywords = ['ØªØ®ØµØµ', 'Ù…ØªØ®ØµØµ', 'ÙÙˆÙ‚ ØªØ®ØµØµ']
            
            for keyword in specialty_keywords:
                elements = soup.find_all(text=re.compile(keyword))
                for element in elements:
                    parent = element.parent
                    if parent:
                        text = parent.get_text().strip()
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ®ØµØµ Ø§Ø² Ù…ØªÙ†
                        if 'Ù…ØªØ®ØµØµ' in text:
                            specialty = re.sub(r'.*Ù…ØªØ®ØµØµ\s*', '', text)
                            specialty = re.sub(r'\s*\(.*\)', '', specialty)
                            if specialty.strip():
                                return specialty.strip()
            
            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± meta tags
            meta_desc = soup.find('meta', {'name': 'description'})
            if meta_desc:
                desc = meta_desc.get('content', '')
                if 'Ù…ØªØ®ØµØµ' in desc:
                    specialty_match = re.search(r'Ù…ØªØ®ØµØµ\s+([^ØŒ\-\(\)]+)', desc)
                    if specialty_match:
                        return specialty_match.group(1).strip()
            
            return "Ø¹Ù…ÙˆÙ…ÛŒ"
            
        except Exception:
            return "Ø¹Ù…ÙˆÙ…ÛŒ"
    
    def _extract_center_info(self, soup: BeautifulSoup) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±Ú©Ø² Ø¯Ø±Ù…Ø§Ù†ÛŒ"""
        try:
            center_info = {
                'center_name': '',
                'center_address': '',
                'center_phone': ''
            }
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù†Ø§Ù… Ù…Ø±Ú©Ø²
            center_keywords = ['Ù…Ø·Ø¨', 'Ú©Ù„ÛŒÙ†ÛŒÚ©', 'Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù†', 'Ù…Ø±Ú©Ø²']
            for keyword in center_keywords:
                elements = soup.find_all(text=re.compile(keyword))
                if elements:
                    center_info['center_name'] = elements[0].strip()
                    break
            
            if not center_info['center_name']:
                center_info['center_name'] = "Ù…Ø±Ú©Ø² Ø¯Ø±Ù…Ø§Ù†ÛŒ"
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¢Ø¯Ø±Ø³
            address_elements = soup.find_all(text=re.compile(r'Ø¢Ø¯Ø±Ø³|Ù†Ø´Ø§Ù†ÛŒ'))
            for element in address_elements:
                parent = element.parent
                if parent:
                    siblings = parent.find_next_siblings()
                    for sibling in siblings:
                        text = sibling.get_text().strip()
                        if len(text) > 10:  # Ø¢Ø¯Ø±Ø³ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø§Ø³Øª
                            center_info['center_address'] = text
                            break
                    if center_info['center_address']:
                        break
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ ØªÙ„ÙÙ†
            phone_pattern = r'(\d{3,4}[-\s]?\d{7,8}|\d{11})'
            phone_elements = soup.find_all(text=re.compile(phone_pattern))
            if phone_elements:
                phone_match = re.search(phone_pattern, phone_elements[0])
                if phone_match:
                    center_info['center_phone'] = phone_match.group(1)
            
            return center_info
            
        except Exception:
            return {
                'center_name': 'Ù…Ø±Ú©Ø² Ø¯Ø±Ù…Ø§Ù†ÛŒ',
                'center_address': '',
                'center_phone': ''
            }
    
    async def _extract_api_info(self, client: httpx.AsyncClient, doctor_url: str, slug: str) -> Optional[Dict]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª API Ø§Ø² Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ AJAX
        
        Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ±ÛŒÙ† Ù‚Ø³Ù…Øª Ø§Ø³Øª Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ reverse engineering Ø¯Ø§Ø±Ø¯
        """
        try:
            # Ø±ÙˆØ´ 1: Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± JavaScript Ú©Ø¯Ù‡Ø§
            response = await client.get(doctor_url, headers=self.headers)
            html = response.text
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª API Ø¯Ø± JavaScript
            api_info = self._extract_from_javascript(html)
            if api_info:
                return api_info
            
            # Ø±ÙˆØ´ 2: ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² API Ø¬Ø³ØªØ¬Ùˆ
            search_api_info = await self._get_from_search_api(client, slug)
            if search_api_info:
                return search_api_info
            
            # Ø±ÙˆØ´ 3: Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ (Ø¨Ø±Ø§ÛŒ ØªØ³Øª)
            return {
                'center_id': '0',
                'service_id': '0', 
                'user_center_id': '0',
                'terminal_id': '0'
            }
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª API: {e}")
            return None
    
    def _extract_from_javascript(self, html: str) -> Optional[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª API Ø§Ø² Ú©Ø¯Ù‡Ø§ÛŒ JavaScript"""
        try:
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¯Ø± JavaScript
            patterns = [
                r'center_id["\']?\s*:\s*["\']?(\d+)',
                r'service_id["\']?\s*:\s*["\']?(\d+)',
                r'user_center_id["\']?\s*:\s*["\']?(\d+)',
                r'terminal_id["\']?\s*:\s*["\']?(\d+)',
                r'centerId["\']?\s*:\s*["\']?(\d+)',
                r'serviceId["\']?\s*:\s*["\']?(\d+)',
            ]
            
            api_info = {}
            
            for pattern in patterns:
                matches = re.findall(pattern, html, re.IGNORECASE)
                if matches:
                    field_name = pattern.split('[')[0].lower()
                    if 'center' in field_name and 'user' not in field_name:
                        api_info['center_id'] = matches[0]
                    elif 'service' in field_name:
                        api_info['service_id'] = matches[0]
                    elif 'user_center' in field_name:
                        api_info['user_center_id'] = matches[0]
                    elif 'terminal' in field_name:
                        api_info['terminal_id'] = matches[0]
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø§Ù…Ù„ Ø¨ÙˆØ¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª
            required_fields = ['center_id', 'service_id', 'user_center_id', 'terminal_id']
            if all(field in api_info for field in required_fields):
                return api_info
            
            return None
            
        except Exception:
            return None
    
    async def _get_from_search_api(self, client: httpx.AsyncClient, slug: str) -> Optional[Dict]:
        """ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² API Ø¬Ø³ØªØ¬Ùˆ"""
        try:
            # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ­Ù‚ÛŒÙ‚ Ø¨ÛŒØ´ØªØ± Ø¯Ø§Ø±Ø¯
            # Ù…Ù…Ú©Ù† Ø§Ø³Øª API Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÙˆÙ…ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
            
            search_url = f"{self.api_base}/search/doctors"
            params = {'q': slug}
            
            response = await client.get(search_url, params=params)
            if response.status_code == 200:
                data = response.json()
                # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ØªØ§ÛŒØ¬...
                pass
            
            return None
            
        except Exception:
            return None
    
    def _parse_search_results(self, soup: BeautifulSoup) -> List[Dict]:
        """Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ"""
        try:
            doctors = []
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ø¯Ú©ØªØ± Ø¯Ø± ØµÙØ­Ù‡
            doctor_cards = soup.find_all(['div', 'article'], class_=re.compile(r'doctor|card|result'))
            
            for card in doctor_cards:
                doctor_info = self._parse_doctor_card(card)
                if doctor_info:
                    doctors.append(doctor_info)
            
            return doctors
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ø±Ø³ Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ: {e}")
            return []
    
    def _parse_doctor_card(self, card) -> Optional[Dict]:
        """Ù¾Ø§Ø±Ø³ ï¿½ï¿½Ø±Ø¯Ù† Ú©Ø§Ø±Øª ÛŒÚ© Ø¯Ú©ØªØ±"""
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©
            link_element = card.find('a', href=re.compile(r'/dr/'))
            if not link_element:
                return None
            
            doctor_url = urljoin(self.base_url, link_element['href'])
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù…
            name_element = card.find(['h1', 'h2', 'h3', 'h4'])
            name = name_element.get_text().strip() if name_element else "Ù†Ø§Ù…Ø´Ø®Øµ"
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ®ØµØµ
            specialty_element = card.find(text=re.compile(r'Ù…ØªØ®ØµØµ|ØªØ®ØµØµ'))
            specialty = specialty_element.strip() if specialty_element else ""
            
            return {
                'name': name,
                'url': doctor_url,
                'specialty': specialty
            }
            
        except Exception:
            return None


# ==================== USAGE EXAMPLE ====================

async def example_usage():
    """Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² API"""
    
    search_api = DoctorSearchAPI()
    
    # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¯Ú©ØªØ±
    doctors = await search_api.search_doctors("Ú©Ø§Ø±Ø¯ÛŒÙˆÙ„ÙˆÚ˜ÛŒ", city="ØªÙ‡Ø±Ø§Ù†")
    print(f"ğŸ” {len(doctors)} Ø¯Ú©ØªØ± Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² URL
    doctor_url = "https://www.paziresh24.com/dr/doctor-name/"
    doctor = await search_api.get_doctor_from_url(doctor_url)
    
    if doctor:
        print(f"âœ… Ø¯Ú©ØªØ± {doctor.name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")
        print(f"   ØªØ®ØµØµ: {doctor.specialty}")
        print(f"   Ù…Ø±Ú©Ø²: {doctor.center_name}")
    else:
        print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ú©ØªØ±")


if __name__ == "__main__":
    import asyncio
    asyncio.run(example_usage())