#!/usr/bin/env python3
"""
ØªØ³Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ú©ØªØ± Ø§Ø² ØµÙØ­Ù‡ Ù¾Ø°ÛŒØ±Ø´Û²Û´
"""
import asyncio
import httpx
import re
import json
from bs4 import BeautifulSoup
from urllib.parse import urlparse


class DoctorInfoExtractor:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ú©ØªØ± Ø§Ø² ØµÙØ­Ù‡ Ù¾Ø°ÛŒØ±Ø´Û²Û´"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fa,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
    
    async def extract_doctor_info(self, doctor_url: str):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ú©ØªØ± Ø§Ø² URL"""
        print(f"ğŸ” ØªØ­Ù„ÛŒÙ„ URL: {doctor_url}")
        
        # 1. Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ URL
        if not self._validate_url(doctor_url):
            print("âŒ URL Ù†Ø§Ù…Ø¹ØªØ¨Ø±")
            return None
        
        # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ slug
        slug = self._extract_slug(doctor_url)
        print(f"ğŸ“ Slug: {slug}")
        
        # 3. Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡
        html_content = await self._fetch_page(doctor_url)
        if not html_content:
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡")
            return None
        
        # 4. ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§
        doctor_info = await self._analyze_page_content(html_content, slug)
        
        return doctor_info
    
    def _validate_url(self, url: str) -> bool:
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ URL"""
        pattern = r'https?://(?:www\.)?paziresh24\.com/dr/[^/]+/?'
        return re.match(pattern, url) is not None
    
    def _extract_slug(self, url: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ slug Ø§Ø² URL"""
        match = re.search(r'/dr/([^/]+)/?', url)
        return match.group(1) if match else ""
    
    async def _fetch_page(self, url: str) -> str:
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡"""
        try:
            async with httpx.AsyncClient(timeout=15) as client:
                response = await client.get(url, headers=self.headers)
                response.raise_for_status()
                return response.text
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡: {e}")
            return ""
    
    async def _analyze_page_content(self, html: str, slug: str) -> dict:
        """ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡"""
        print("\nğŸ” Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§...")
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡
        basic_info = self._extract_basic_info(soup, slug)
        print(f"âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡: {basic_info}")
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª API
        api_info = self._search_api_info(html, soup)
        print(f"ğŸ”‘ Ø§Ø·Ù„Ø§Ø¹Ø§Øª API: {api_info}")
        
        # ØªØ±Ú©ÛŒØ¨ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
        result = {**basic_info, **api_info}
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
        self._display_results(result)
        
        return result
    
    def _extract_basic_info(self, soup: BeautifulSoup, slug: str) -> dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡"""
        info = {'slug': slug}
        
        # Ù†Ø§Ù… Ø¯Ú©ØªØ±
        name_selectors = [
            'h1',
            '[data-testid="doctor-name"]',
            '.doctor-name',
            'title'
        ]
        
        for selector in name_selectors:
            element = soup.select_one(selector)
            if element:
                name = element.get_text().strip()
                name = re.sub(r'^(Ø¯Ú©ØªØ±|Dr\.?)\s*', '', name)
                name = re.sub(r'\s*-.*$', '', name)
                if name:
                    info['name'] = name
                    break
        
        # ØªØ®ØµØµ
        specialty_patterns = [
            r'Ù…ØªØ®ØµØµ\s+([^ØŒ\-\(\)]+)',
            r'ØªØ®ØµØµ[:\s]*([^ØŒ\-\(\)]+)',
            r'specialty["\']?\s*:\s*["\']([^"\']+)'
        ]
        
        for pattern in specialty_patterns:
            matches = re.findall(pattern, str(soup), re.IGNORECASE)
            if matches:
                info['specialty'] = matches[0].strip()
                break
        
        # Ù†Ø§Ù… Ù…Ø±Ú©Ø²
        center_patterns = [
            r'(Ù…Ø·Ø¨|Ú©Ù„ÛŒÙ†ÛŒÚ©|Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù†|Ù…Ø±Ú©Ø²)\s+([^ØŒ\-\(\)]+)',
            r'center["\']?\s*:\s*["\']([^"\']+)'
        ]
        
        for pattern in center_patterns:
            matches = re.findall(pattern, str(soup), re.IGNORECASE)
            if matches:
                info['center_name'] = matches[0] if isinstance(matches[0], str) else matches[0][1]
                break
        
        return info
    
    def _search_api_info(self, html: str, soup: BeautifulSoup) -> dict:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª API"""
        api_info = {}
        
        print("\nğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª API...")
        
        # 1. Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± JavaScript variables
        js_patterns = [
            r'center_id["\']?\s*:\s*["\']?(\d+)',
            r'service_id["\']?\s*:\s*["\']?(\d+)',
            r'user_center_id["\']?\s*:\s*["\']?(\d+)',
            r'terminal_id["\']?\s*:\s*["\']?(\d+)',
            r'centerId["\']?\s*:\s*["\']?(\d+)',
            r'serviceId["\']?\s*:\s*["\']?(\d+)',
        ]
        
        for pattern in js_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            if matches:
                field_name = pattern.split('[')[0].lower()
                if 'center' in field_name and 'user' not in field_name:
                    api_info['center_id'] = matches[0]
                    print(f"  âœ… center_id: {matches[0]}")
                elif 'service' in field_name:
                    api_info['service_id'] = matches[0]
                    print(f"  âœ… service_id: {matches[0]}")
                elif 'user_center' in field_name:
                    api_info['user_center_id'] = matches[0]
                    print(f"  âœ… user_center_id: {matches[0]}")
                elif 'terminal' in field_name:
                    api_info['terminal_id'] = matches[0]
                    print(f"  âœ… terminal_id: {matches[0]}")
        
        # 2. Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± JSON objects
        json_patterns = [
            r'doctorData\s*=\s*({[^}]+})',
            r'appointmentData\s*=\s*({[^}]+})',
            r'bookingConfig\s*=\s*({[^}]+})',
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            for match in matches:
                try:
                    data = json.loads(match)
                    print(f"  ğŸ“‹ JSON object found: {data}")
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² JSON
                    for key in ['center_id', 'service_id', 'user_center_id', 'terminal_id']:
                        if key in data:
                            api_info[key] = str(data[key])
                            print(f"  âœ… {key}: {data[key]}")
                            
                except json.JSONDecodeError:
                    continue
        
        # 3. Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± data attributes
        data_attrs = soup.find_all(attrs=lambda x: x and any(
            attr.startswith('data-') and any(keyword in attr for keyword in ['center', 'service', 'terminal'])
            for attr in x.keys()
        ))
        
        for element in data_attrs:
            print(f"  ğŸ“‹ Data attribute element: {element.attrs}")
            for attr, value in element.attrs.items():
                if 'center' in attr and 'user' not in attr:
                    api_info['center_id'] = str(value)
                elif 'service' in attr:
                    api_info['service_id'] = str(value)
                elif 'user_center' in attr or 'user-center' in attr:
                    api_info['user_center_id'] = str(value)
                elif 'terminal' in attr:
                    api_info['terminal_id'] = str(value)
        
        return api_info
    
    def _display_results(self, result: dict):
        """Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬"""
        print("\n" + "="*50)
        print("ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬:")
        print("="*50)
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡
        print("\nâœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡:")
        for key in ['name', 'slug', 'specialty', 'center_name']:
            value = result.get(key, 'ÛŒØ§ÙØª Ù†Ø´Ø¯')
            print(f"  {key}: {value}")
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª API
        print("\nğŸ”‘ Ø§Ø·Ù„Ø§Ø¹Ø§Øª API:")
        api_fields = ['center_id', 'service_id', 'user_center_id', 'terminal_id']
        found_api_fields = 0
        
        for field in api_fields:
            value = result.get(field, 'âŒ ÛŒØ§ÙØª Ù†Ø´Ø¯')
            status = "âœ…" if field in result else "âŒ"
            print(f"  {status} {field}: {value}")
            if field in result:
                found_api_fields += 1
        
        # Ø®Ù„Ø§ØµÙ‡
        print(f"\nğŸ“ˆ Ø®Ù„Ø§ØµÙ‡: {found_api_fields}/{len(api_fields)} ÙÛŒÙ„Ø¯ API ÛŒØ§ÙØª Ø´Ø¯")
        
        if found_api_fields == len(api_fields):
            print("ğŸ‰ ØªÙ…Ø§Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² ÛŒØ§ÙØª Ø´Ø¯!")
        elif found_api_fields > 0:
            print("âš ï¸ Ø¨Ø±Ø®ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª API ÛŒØ§ÙØª Ø´Ø¯ØŒ ÙˆÙ„ÛŒ Ù†Ø§Ù‚Øµ Ø§Ø³Øª")
        else:
            print("âŒ Ù‡ÛŒÚ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª API ÛŒØ§ÙØª Ù†Ø´Ø¯")
        
        print("="*50)


async def main():
    """ØªØ³Øª Ø§ØµÙ„ÛŒ"""
    print("ğŸ§ª ØªØ³Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ú©ØªØ± Ø§Ø² Ù¾Ø°ÛŒØ±Ø´Û²Û´")
    print("="*50)
    
    # URL Ù†Ù…ÙˆÙ†Ù‡ - Ø´Ù…Ø§ URL ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯
    test_urls = [
        "https://www.paziresh24.com/dr/Ø¯Ú©ØªØ±-Ø³ÛŒØ¯Ù…Ø­Ù…Ø¯Ù…Ø¬ØªØ¨ÛŒ-Ù…ÙˆØ³ÙˆÛŒ-0/",
        "https://www.paziresh24.com/dr/Ø¯Ú©ØªØ±-Ù…Ø­Ù…Ø¯Ø±Ø¶Ø§-Ú©Ø±ÛŒÙ…ÛŒ-1/",
        "https://www.paziresh24.com/dr/Ø¯Ú©ØªØ±-ÙØ§Ø·Ù…Ù‡-Ø§Ø­Ù…Ø¯ÛŒ-2/",
        # Ø´Ù…Ø§ URL Ù‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
    ]
    
    print("ğŸ’¡ Ù†Ú©ØªÙ‡: Ø§Ú¯Ø± URL Ø®Ø§ØµÛŒ Ø¯Ø§Ø±ÛŒØ¯ØŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ø¬Ø§ÛŒ URL Ù‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯")
    print("ğŸ“ ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ URL Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù† ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:")
    print("   python test_doctor_extraction.py 'https://www.paziresh24.com/dr/doctor-name/'")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù† Ø®Ø· ÙØ±Ù…Ø§Ù†
    import sys
    if len(sys.argv) > 1:
        custom_url = sys.argv[1]
        if custom_url.startswith('http'):
            test_urls = [custom_url]
            print(f"ğŸ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² URL Ø³ÙØ§Ø±Ø´ÛŒ: {custom_url}")
    
    extractor = DoctorInfoExtractor()
    
    for url in test_urls:
        print(f"\nğŸ” ØªØ³Øª URL: {url}")
        result = await extractor.extract_doctor_info(url)
        
        if result:
            print("âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆÙÙ‚")
        else:
            print("âŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù…ÙˆÙÙ‚")
        
        print("-" * 50)


if __name__ == "__main__":
    asyncio.run(main())